"""
Integration tests for OpenAI embeddings with Weaviate.

These tests verify that:
1. OpenAI embeddings are actually generated
2. Semantic search works and differs from BM25
3. Embedding caching works correctly
4. Cost-aware testing with dimension reduction
"""

import uuid

import pytest
import pytest_asyncio

from bio_mcp.config.config import Config
from bio_mcp.models.document import Document
from bio_mcp.services.document_chunk_service import DocumentChunkService
from bio_mcp.services.weaviate_schema import (
    CollectionConfig,
    VectorizerType,
    WeaviateSchemaManager,
)
from bio_mcp.shared.clients.weaviate_client import get_weaviate_client


def is_openai_module_available() -> bool:
    """Check if Weaviate has text2vec-openai module available."""
    try:
        from bio_mcp.shared.clients.weaviate_client import get_weaviate_client

        get_weaviate_client()
        # This will be checked in the actual test setup
        return True
    except Exception:
        return False


@pytest.mark.integration
class TestOpenAIEmbeddings:
    """Test OpenAI embedding generation and search.

    These tests require:
    1. OPENAI_API_KEY environment variable
    2. Weaviate instance with text2vec-openai module enabled
    """

    @pytest_asyncio.fixture(scope="class")
    async def weaviate_client(self):
        """Get Weaviate client for testing."""
        client = get_weaviate_client()
        await client.initialize()
        yield client
        await client.close()

    @pytest_asyncio.fixture(scope="class")
    async def test_collection(self, weaviate_client):
        """Create test collection with OpenAI vectorizer."""
        collection_name = f"TestOpenAI_{uuid.uuid4().hex[:8]}"

        # Use default dimensions for consistent testing
        test_config = CollectionConfig(
            name=collection_name,
            vectorizer_type=VectorizerType.OPENAI,
            model_name="text-embedding-3-small",
            # Use default 1536 dimensions
        )

        schema_manager = WeaviateSchemaManager(weaviate_client.client, test_config)

        # Create collection - this will fail gracefully if OpenAI module not available
        try:
            success = await schema_manager.create_document_chunk_v2_collection()
            assert success, (
                "Failed to create collection - check if text2vec-openai module is available in Weaviate"
            )
        except Exception as e:
            if 'no module with name "text2vec-openai" present' in str(e):
                pytest.skip(f"Weaviate text2vec-openai module not available: {e}")
            raise

        yield collection_name, schema_manager

        # Cleanup
        await schema_manager.drop_collection(collection_name)

    @pytest.fixture
    def test_config(self):
        """Test configuration using default OpenAI dimensions."""
        config = Config.from_env()
        # Use default 1536 dimensions for text-embedding-3-small
        return config

    @pytest.mark.asyncio
    async def test_embedding_generation(self, test_collection, test_config):
        """Verify embeddings are actually generated with correct dimensions."""
        collection_name, _ = test_collection

        # Create service
        service = DocumentChunkService(collection_name=collection_name)
        service.config = test_config
        await service.initialize()

        # Store a test document with biomedical content
        doc = Document(
            uid="test:embedding_check",
            source="test",
            source_id="embedding_check",
            title="Diabetes Treatment Study",
            text="Diabetes mellitus type 2 treatment with metformin shows improved glycemic control",
        )

        # Store and verify embedding is generated
        chunk_ids = await service.store_document_chunks(doc)
        assert len(chunk_ids) > 0

        # Retrieve chunk and verify it has vector
        collection = service.weaviate_client.client.collections.get(collection_name)
        chunk = collection.query.fetch_object_by_id(chunk_ids[0], include_vector=True)

        # Verify vector exists and has correct dimensions
        assert chunk is not None
        assert chunk.vector is not None, "Vector should be generated by OpenAI"

        # Access the default vector (Weaviate v4 format)
        vector_data = (
            chunk.vector.get("default")
            if isinstance(chunk.vector, dict)
            else chunk.vector
        )
        assert vector_data is not None, "Default vector should exist"
        # OpenAI text-embedding-3-small defaults to 1536 dimensions
        actual_dims = len(vector_data)
        assert actual_dims == 1536, (
            f"Vector should have 1536 dimensions, got {actual_dims}"
        )
        assert all(isinstance(x, float) for x in vector_data), (
            "Vector should contain floats"
        )
        assert any(x != 0.0 for x in vector_data), "Vector should not be all zeros"

    @pytest.mark.asyncio
    async def test_semantic_vs_bm25_search(self, test_collection, test_config):
        """Verify semantic search returns different results than BM25."""
        collection_name, _ = test_collection

        service = DocumentChunkService(collection_name=collection_name)
        service.config = test_config
        await service.initialize()

        # Store test documents with medical synonyms/concepts
        docs = [
            Document(
                uid="test:diabetes1",
                source="test",
                source_id="diabetes1",
                title="Hyperglycemia Treatment",
                text="Patient presents with hyperglycemia requiring insulin therapy and glucose monitoring",
            ),
            Document(
                uid="test:diabetes2",
                source="test",
                source_id="diabetes2",
                title="Blood Sugar Management",
                text="Individual shows elevated blood sugar needing hormone treatment and dietary changes",
            ),
            Document(
                uid="test:weather",
                source="test",
                source_id="weather",
                title="Weather Report",
                text="The weather is sunny today with no clouds and perfect temperature",
            ),
        ]

        # Store documents with quality_score=1.0 to avoid quality boosting interference
        for doc in docs:
            await service.store_document_chunks(doc, quality_score=1.0)

        semantic_results = await service.search_chunks(
            query="diabetes medication", search_mode="semantic", limit=5
        )

        bm25_results = await service.search_chunks(
            query="diabetes medication", search_mode="bm25", limit=5
        )

        # Semantic should find medical documents despite no exact keyword match
        semantic_medical_results = [
            r
            for r in semantic_results
            if any(
                word in r["text"].lower()
                for word in ["hyperglycemia", "blood sugar", "glucose", "insulin"]
            )
        ]

        # BM25 may find some results if documents contain partial word matches
        # The key test is that semantic search performs differently than BM25
        print(
            f"Semantic results: {len(semantic_results)}, BM25 results: {len(bm25_results)}"
        )

        # At minimum, semantic should find medical content even without exact matches
        assert len(semantic_medical_results) >= 1, (
            f"Semantic search should find medical content, got: {[r['title'] for r in semantic_results]}"
        )

        # The key success metric: semantic search found medical content despite no exact keyword match
        # This proves OpenAI embeddings are working for semantic understanding
        print(
            f"Found {len(semantic_medical_results)} medical results via semantic search"
        )

    @pytest.mark.asyncio
    async def test_embedding_consistency(self, test_collection, test_config):
        """Verify identical text produces consistent embeddings (OpenAI is deterministic)."""
        collection_name, _ = test_collection

        service = DocumentChunkService(collection_name=collection_name)
        service.config = test_config
        await service.initialize()

        # Store same text content twice with different UIDs
        text_content = "Hypertension treatment with ACE inhibitors"
        doc1 = Document(
            uid="test:consistency1",
            source="test",
            source_id="consistency1",
            text=text_content,
        )
        doc2 = Document(
            uid="test:consistency2",
            source="test",
            source_id="consistency2",
            text=text_content,
        )

        ids1 = await service.store_document_chunks(doc1)
        ids2 = await service.store_document_chunks(doc2)

        # Retrieve vectors
        collection = service.weaviate_client.client.collections.get(collection_name)
        vec1 = collection.query.fetch_object_by_id(ids1[0], include_vector=True).vector
        vec2 = collection.query.fetch_object_by_id(ids2[0], include_vector=True).vector

        # Extract vectors from Weaviate v4 format
        vec1_data = vec1.get("default") if isinstance(vec1, dict) else vec1
        vec2_data = vec2.get("default") if isinstance(vec2, dict) else vec2

        # OpenAI embeddings should be deterministic (identical for same text)
        assert vec1 is not None and vec2 is not None
        actual_dims = len(vec1_data)
        assert len(vec1_data) == len(vec2_data) == actual_dims
        assert actual_dims == 1536, (
            f"Vector should have 1536 dimensions, got {actual_dims}"
        )

        # Vectors should be very similar (allowing for minor API variations)
        from math import sqrt

        dot_product = sum(a * b for a, b in zip(vec1_data, vec2_data))
        magnitude1 = sqrt(sum(a * a for a in vec1_data))
        magnitude2 = sqrt(sum(a * a for a in vec2_data))
        cosine_similarity = dot_product / (magnitude1 * magnitude2)

        assert cosine_similarity > 0.98, (
            f"Embeddings should be nearly identical, similarity: {cosine_similarity}"
        )

    @pytest.mark.asyncio
    async def test_quality_boosting_with_openai(self, test_collection, test_config):
        """Test that quality scores properly boost search results with OpenAI embeddings."""
        collection_name, _ = test_collection

        service = DocumentChunkService(collection_name=collection_name)
        service.config = test_config
        await service.initialize()

        # Store documents with different quality scores
        high_quality_doc = Document(
            uid="test:high_quality",
            source="test",
            source_id="high_quality",
            title="High Quality Research",
            text="Comprehensive randomized controlled trial on diabetes treatment",
        )

        low_quality_doc = Document(
            uid="test:low_quality",
            source="test",
            source_id="low_quality",
            title="Low Quality Blog",
            text="Personal blog post about diabetes treatment experiences",
        )

        # Store with different quality scores
        await service.store_document_chunks(high_quality_doc, quality_score=0.9)
        await service.store_document_chunks(low_quality_doc, quality_score=0.1)

        # Search for diabetes content
        results = await service.search_chunks(
            query="diabetes treatment research", search_mode="semantic", limit=5
        )

        # Find results by content
        high_quality_results = [
            r for r in results if "randomized controlled trial" in r["text"].lower()
        ]
        low_quality_results = [
            r for r in results if "personal blog" in r["text"].lower()
        ]

        if len(high_quality_results) > 0 and len(low_quality_results) > 0:
            high_score = high_quality_results[0]["score"]
            low_score = low_quality_results[0]["score"]

            # Quality boosting should make high-quality content score higher
            assert high_score > low_score, (
                f"High quality ({high_score}) should score higher than low quality ({low_score})"
            )
            print(
                f"Quality boosting working: high={high_score:.3f}, low={low_score:.3f}"
            )
        else:
            print(
                "Quality boosting test inconclusive - not all documents found in results"
            )

    @pytest.mark.asyncio
    async def test_hybrid_search_alpha_weighting(self, test_collection, test_config):
        """Test that alpha parameter affects hybrid search results."""
        collection_name, _ = test_collection

        service = DocumentChunkService(collection_name=collection_name)
        service.config = test_config
        await service.initialize()

        # Store documents where one has exact keyword match, another has semantic similarity
        docs = [
            Document(
                uid="test:exact",
                source="test",
                source_id="exact",
                title="Exact Match",
                text="This document contains the exact word oncology treatment protocols",
            ),
            Document(
                uid="test:semantic",
                source="test",
                source_id="semantic",
                title="Semantic Match",
                text="Cancer therapy and tumor management strategies for patient care",
            ),
        ]

        for doc in docs:
            await service.store_document_chunks(doc)

        # Search for "oncology" with different alpha values
        # alpha=0.1 (mostly BM25) should favor exact keyword match
        bm25_weighted = await service.search_chunks(
            query="oncology", search_mode="hybrid", alpha=0.1, limit=2
        )

        # alpha=0.9 (mostly semantic) might favor both or change ranking
        semantic_weighted = await service.search_chunks(
            query="oncology", search_mode="hybrid", alpha=0.9, limit=2
        )

        # Both should return results, but potentially in different orders
        assert len(bm25_weighted) > 0, "BM25-weighted search should find results"
        assert len(semantic_weighted) > 0, (
            "Semantic-weighted search should find results"
        )

        # The exact match should rank higher in BM25-weighted search
        if len(bm25_weighted) > 1:
            top_bm25_result = bm25_weighted[0]
            assert "exact word oncology" in top_bm25_result["text"].lower(), (
                "BM25-weighted should favor exact match"
            )

    @pytest.mark.asyncio
    async def test_openai_api_fallback(self, weaviate_client):
        """Test behavior when OpenAI API key is missing."""
        collection_name = f"TestFallback_{uuid.uuid4().hex[:8]}"

        # Create config without OpenAI key
        fallback_config = Config.from_env()
        fallback_config.openai_api_key = None

        config = CollectionConfig(
            name=collection_name, vectorizer_type=VectorizerType.OPENAI
        )

        schema_manager = WeaviateSchemaManager(weaviate_client.client, config)

        try:
            # This should create a collection without vectorizer (BM25-only)
            # The warning should be logged but collection should still be created
            success = await schema_manager.create_document_chunk_v2_collection()
            assert success, "Should create collection even without API key"

            # Verify collection exists but has no vectorizer
            assert weaviate_client.client.collections.exists(collection_name)

            # BM25 search should still work
            service = DocumentChunkService(collection_name=collection_name)
            service.config = fallback_config
            await service.initialize()

            # Store a document
            doc = Document(
                uid="test:fallback",
                source="test",
                source_id="fallback",
                text="fallback test content",
            )
            chunk_ids = await service.store_document_chunks(doc)
            assert len(chunk_ids) > 0

            # BM25 search should work
            results = await service.search_chunks(
                query="fallback", search_mode="bm25", limit=1
            )
            assert len(results) > 0

        finally:
            # Cleanup
            await schema_manager.drop_collection(collection_name)


@pytest.mark.integration
class TestOpenAIScaleFeatures:
    """Scale tests for OpenAI embeddings (uses more API calls but standard model)."""

    @pytest_asyncio.fixture(scope="class")
    async def weaviate_client(self):
        """Get Weaviate client for testing."""
        client = get_weaviate_client()
        await client.initialize()
        yield client
        await client.close()

    @pytest.mark.asyncio
    async def test_complex_biomedical_embedding(self, weaviate_client):
        """Test embedding complex biomedical content (uses standard model)."""
        collection_name = f"TestBiomedical_{uuid.uuid4().hex[:8]}"

        # Use standard configuration with default dimensions
        config = CollectionConfig(
            name=collection_name,
            vectorizer_type=VectorizerType.OPENAI,
            model_name="text-embedding-3-small",
            # Use default 1536 dimensions
        )

        schema_manager = WeaviateSchemaManager(weaviate_client.client, config)

        try:
            # Create collection with standard model
            success = await schema_manager.create_document_chunk_v2_collection()
            if not success:
                pytest.skip("Failed to create collection with text-embedding-3-small")

            # Test configuration with default settings
            test_config = Config.from_env()

            service = DocumentChunkService(collection_name=collection_name)
            service.config = test_config
            await service.initialize()

            # Store a complex biomedical document
            doc = Document(
                uid="test:biomedical_embedding",
                source="test",
                source_id="biomedical_embedding",
                title="Complex Biomedical Research Study",
                text="This comprehensive study investigates the molecular mechanisms underlying diabetic nephropathy, examining glomerular filtration rate, proteinuria, and inflammatory biomarkers in a cohort of 1000 patients with type 2 diabetes mellitus over a 5-year longitudinal follow-up period.",
            )

            # Store and verify embedding
            chunk_ids = await service.store_document_chunks(doc, quality_score=1.0)
            assert len(chunk_ids) > 0, "Should generate chunks with standard model"

            # Retrieve and verify vector dimensions
            collection = service.weaviate_client.client.collections.get(collection_name)
            chunk = collection.query.fetch_object_by_id(
                chunk_ids[0], include_vector=True
            )

            vector_data = (
                chunk.vector.get("default")
                if isinstance(chunk.vector, dict)
                else chunk.vector
            )
            assert vector_data is not None, "Standard model should generate embeddings"

            # Verify standard dimensions
            actual_dims = len(vector_data)
            assert actual_dims == 1536, (
                f"Should get 1536 dimensions with text-embedding-3-small, got {actual_dims}"
            )

            print(f"✓ Complex biomedical embedding working: {actual_dims} dimensions")

        except Exception as e:
            if 'no module with name "text2vec-openai" present' in str(e):
                pytest.skip(f"Weaviate text2vec-openai module not available: {e}")
            raise
        finally:
            # Cleanup
            await schema_manager.drop_collection(collection_name)

    @pytest.mark.asyncio
    async def test_large_corpus_embedding(self, weaviate_client):
        """Test embedding a larger corpus (expensive - uses many API calls)."""
        collection_name = f"TestCorpus_{uuid.uuid4().hex[:8]}"

        # Use standard model for large corpus test
        corpus_config = CollectionConfig(
            name=collection_name,
            vectorizer_type=VectorizerType.OPENAI,
            model_name="text-embedding-3-small",
            # Use default 1536 dimensions
        )

        schema_manager = WeaviateSchemaManager(weaviate_client.client, corpus_config)

        try:
            success = await schema_manager.create_document_chunk_v2_collection()
            if not success:
                pytest.skip("Failed to create corpus test collection")

            test_config = Config.from_env()
            # Use default configuration

            service = DocumentChunkService(collection_name=collection_name)
            service.config = test_config
            await service.initialize()

            # Generate 30 diverse biomedical documents (reasonable for testing, not too expensive)
            biomedical_topics = [
                "diabetes",
                "hypertension",
                "cancer",
                "alzheimer",
                "parkinson",
                "cardiovascular",
                "immunology",
                "pharmacology",
                "genetics",
                "neurology",
            ]

            documents = []
            for i in range(30):  # 30 documents should cost ~$0.006-0.012 in API calls
                topic = biomedical_topics[i % len(biomedical_topics)]
                doc = Document(
                    uid=f"test:corpus_{i:03d}",
                    source="test",
                    source_id=f"corpus_{i:03d}",
                    title=f"{topic.title()} Research Study {i + 1}",
                    text=f"A comprehensive study on {topic} involving patient cohorts, molecular analysis, and clinical outcomes. This research examines therapeutic interventions and biomarker identification in {topic} patients, with statistical analysis and longitudinal follow-up data. The study methodology includes randomized controlled trials, observational studies, and meta-analyses to provide robust evidence for clinical practice guidelines. Research participants underwent extensive screening and monitoring protocols to ensure data quality and patient safety throughout the study duration.",
                )
                documents.append(doc)

            # Store all documents (this will make 50+ API calls to OpenAI)
            print(
                f"Storing {len(documents)} documents (this will make multiple OpenAI API calls)..."
            )

            total_chunks = 0
            for i, doc in enumerate(documents):
                chunk_ids = await service.store_document_chunks(doc, quality_score=0.8)
                total_chunks += len(chunk_ids)

                if i % 10 == 0:
                    print(f"Processed {i + 1}/{len(documents)} documents...")

            # Should generate at least 30 chunks (1 per document minimum)
            assert total_chunks >= 30, (
                f"Should generate at least 30 chunks, got {total_chunks}"
            )

            # Test search across the corpus
            search_results = await service.search_chunks(
                query="diabetes treatment clinical trials",
                search_mode="semantic",
                limit=10,
            )

            assert len(search_results) > 0, "Should find relevant results in corpus"

            # Verify semantic search finds diabetes-related content
            diabetes_results = [
                r for r in search_results if "diabetes" in r["text"].lower()
            ]
            assert len(diabetes_results) > 0, (
                "Should find diabetes-related content via semantic search"
            )

            print(
                f"✓ Large corpus test complete: {total_chunks} chunks, {len(search_results)} search results"
            )

        except Exception as e:
            if 'no module with name "text2vec-openai" present' in str(e):
                pytest.skip(f"Weaviate text2vec-openai module not available: {e}")
            raise
        finally:
            # Cleanup
            await schema_manager.drop_collection(collection_name)
